{"cells":[{"metadata":{"_uuid":"eafd3a43c45d13bceb63ea3fc3faead92fc3e072"},"cell_type":"markdown","source":"######  Abalone Data-set - > Extension of EDA Kernel by [Rageeni Sah](https://www.kaggle.com/ragnisah/eda-abalone-age-prediction)\n\n- Model Insights \n- Different Classification Algorithms used,\n- Work Done by [Sriram Arvind Lakshmanakumar](https://www.kaggle.com/sriram1204), [Nikhita Agarwal](https://www.kaggle.com/nikhitaagr)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"524c72fe364d8c00f5620f522fba3b8fc5cd55f3"},"cell_type":"code","source":"''' Library Import'''\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"85231f2bd1089666c673a211f96c00b7b8e3cad3"},"cell_type":"code","source":"''' SK-Learn Library Import'''\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RandomizedLasso,LassoLarsCV\nfrom sklearn.exceptions import ConvergenceWarning \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer, accuracy_score, confusion_matrix\nimport sklearn.datasets ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6a23f2cc41b2340eeab12a04289e4a510ecfd47c"},"cell_type":"code","source":"'''Scipy, Stats Library'''\nfrom scipy.stats import skew","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"23ace09f2322081614f46ee8eb36b9d232bb8be8"},"cell_type":"code","source":"''' To Ignore Warning'''\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c9c68fbddd3306874316703347e9467d1aeefa00"},"cell_type":"code","source":"''' To Do : Inline Priting of Visualizations '''\nsns.set()\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6116abad90a124e1a9eebcd21ea3f28667390837"},"cell_type":"code","source":"''' Importing Data : from the Archive Directly'''\ndf = pd.read_csv(r'../input/abalone.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fb559ebe9a4a52216222002405bea8c0c509a460"},"cell_type":"code","source":"'''Display The head --> To Check if Data is Properly Imported'''\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6d9895d9eda4f7ee0a693675bb63b96168f88736"},"cell_type":"code","source":"''' Feature Information of the DataSet'''\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93ae37334bf0c86d3920e8a89d0d28ab2f938e76"},"cell_type":"markdown","source":"##### According to the Infomation: \n\n- 1)No-Null data\n- 2)1 - Object Type\n- 3)7 - Float Type\n- 4)1 - Int Type"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5579dbef520eb18d806e65aa23d5e3cbcf3656f1"},"cell_type":"code","source":"'''Feature Distirbution of data for Float and Int Data Type'''\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21d38791219fdb560f67a339baaeec17810dc574"},"cell_type":"markdown","source":"###### According to Described Information: \n\n- 1)No Feature has Minimum Value = 0, except *Height*\n- 2)All Features are not Normally Distributed, ( Theortically if feature is normally distributed, Mean = Median = Mode ).\n- 3)But Features are close to Normality\n- 4)All numerical, Except Sex\n- 5)Each Feature has Different Scale"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b67323b3ab6c8c85544df46d21dc5e592457b281"},"cell_type":"code","source":"'''Numerical Features and Categorical Features'''\nnf = df.select_dtypes(include=[np.number]).columns\ncf = df.select_dtypes(include=[np.object]).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"db1a5817bfd36660f480b87841a55f2bcda6c715"},"cell_type":"code","source":"'''List of Numerical Features'''\nnf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"671a7faf59a404a916f5d29eb2344c2b5316445d"},"cell_type":"code","source":"''' List of Categorical Features'''\ncf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"013e05034731441269f271e761e4329b4aa8d8bc"},"cell_type":"code","source":"'''Histogram : to see the numeric data distribution'''\ndf.hist(figsize=(20,20), grid = True, layout = (2,4), bins = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6e48302946d329e24f5f1de8c0c35656f689297b"},"cell_type":"code","source":"'''After Seeing Above Graph of Data Distribution, I feel the Data is skewed, So checking for Skewness '''\nskew_list = skew(df[nf],nan_policy='omit') #sending all numericalfeatures and omitting nan values\nskew_list_df = pd.concat([pd.DataFrame(nf,columns=['Features']),pd.DataFrame(skew_list,columns=['Skewness'])],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"256f2eac13d10905367b6c45d8a5ecf8f3aeb42b"},"cell_type":"code","source":"skew_list_df.sort_values(by='Skewness', ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46ac85de0317800997e70a83914be708c1694812"},"cell_type":"markdown","source":"###### According to the rules\n- For a normally Distributed Data, Skewness should be greater than 0\n- Skewness > 0 , More weight is on the right tail of the distribution\n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bd5850b386f9f506c37f4352e241654a93b9606c"},"cell_type":"code","source":"'''Missing Values '''\nmv_df = df.isnull().sum().sort_values(ascending = False)\npmv_df = (mv_df/len(df)) * 100\nmissing_df = pd.concat([mv_df,pmv_df], axis = 1, keys = ['Missing Values','% Missing'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b9a3909278e047fd75b5eb160ea4a8536f1d44e6"},"cell_type":"code","source":"missing_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b60ff242b5738f00a78d3306f5eea4b35bbe8a7f"},"cell_type":"code","source":"'''Target Column Analysis'''\nprint(\"Value Count of Rings Column\")\nprint(df.Rings.value_counts())\nprint(\"\\nPercentage of Rings Column\")\nprint(df.Rings.value_counts(normalize = True))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9751a8ce91cb1faeb2a75c2d075d6fd44effaf31"},"cell_type":"markdown","source":"###### No of Classes In Target"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2b5ea42757e3a3a15b01678ac24d53f65028fc57"},"cell_type":"code","source":"print(len(df.Rings.unique()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edee12ba90337aee7a6757e49a942b7cbf2fd8da"},"cell_type":"markdown","source":"### Visualization"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b367ec1f34814d9d0dae329a9bea5066a460fd98"},"cell_type":"code","source":"'''Sex Count of Abalone, M - Male, F - Female, I - Infant'''\nsns.countplot(x='Sex', data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"257800d502be702f3195998da4074da424446dfb"},"cell_type":"code","source":"'''Sex Ratio in Abalone'''\nprint(\"\\nSex Count in Percentage\")\nprint(df.Sex.value_counts(normalize = True))\nprint(\"\\nSex Count in Numbers\")\nprint(df.Sex.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4ee4ec5f3e151a2c207ece74cd4d9e7c77a1e52f"},"cell_type":"code","source":"'''Small Feature Engineering, Deriving Age from Rings Column, Age = Rings + 1.5'''\ndf['Age'] = df['Rings'] + 1.5\ndf['Age'].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4a5cc84504aa089fd0236ba809c8390d497cf440"},"cell_type":"code","source":"'''Sex and Age Visulization'''\nplt.figure(figsize = (20,7))\nsns.swarmplot(x = 'Sex', y = 'Age', data = df, hue = 'Sex')\nsns.violinplot(x = 'Sex', y = 'Age', data = df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"356309e484ca7143c055d5d77c08e7210ede9ac5"},"cell_type":"markdown","source":"###### According to The above Graph\n- Male : Majority Between 7.5 to 19\n- Female : Majority Between 8 to 19\n- Infant : Majority Between 6 to < 10"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"62f0aa126b7a1ed9f3800ed8af584736c3bfadc1"},"cell_type":"code","source":"df.groupby('Sex')[['Length', 'Diameter', 'Height', 'Whole weight', \n                   'Shucked weight','Viscera weight', 'Shell weight', 'Age']].mean().sort_values(by = 'Age',ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7044f5e6e4f10041d8e6bc41cd28f9964fbb2f47"},"cell_type":"markdown","source":"   ###### Preprocessing Data for the Model"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c1a9e5bf3becffd852ff87f14ee40ff249719d32"},"cell_type":"code","source":"'''LabelEnconding the Categorical Data'''\ndf['Sex'] = LabelEncoder().fit_transform(df['Sex'].tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a745dab42d0cd59b2a998c879759565512903cd6"},"cell_type":"code","source":"'''One Hot Encoding for Sex Feature '''\ntransformed_sex_feature = OneHotEncoder().fit_transform(df['Sex'].values.reshape(-1,1)).toarray()\ndf_sex_encoded = pd.DataFrame(transformed_sex_feature, columns = [\"Sex_\"+str(int(i)) for i in range(transformed_sex_feature.shape[1])])\ndf = pd.concat([df, df_sex_encoded], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"304c3792923f5d2a7b6f2f875411c80c89d4641b"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3959b8bb2d19efa4464df43905b2244401d2c64b"},"cell_type":"markdown","source":"###### Data Splitting for Model\n- Learning Features\n- Predicting Feature\n- Train & Test Split"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"64a693e85a6fac55499bb7c6dd33e009c7adb060"},"cell_type":"code","source":"'''Learning Features and Predicting Features'''\nXtrain = df.drop(['Rings','Age','Sex'], axis = 1)\nYtrain = df['Rings']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fa9a6a6a1f66ea6defb2a3b7ebe887d5360af9b9"},"cell_type":"code","source":"'''Train Test Split , 70:30 Ratio'''\nX_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59969918fda8858a49586d48e013ecfd24ac20fa"},"cell_type":"markdown","source":"###### Simple Logistic Regression Model\nNo of Classes : 28"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9d3cedaa8cfbcf8eabdeefc2aa77c147e859de17"},"cell_type":"code","source":"'''Creating Object of LogisticRegression'''\nlogreg = LogisticRegression()\n'''Learning from Training Set'''\nlogreg.fit(X_train, Y_train)\n'''Predicting for Training Set'''\nY_pred = logreg.predict(X_test)\n'''Accuracy Score'''\nresult_acc = accuracy_score(Y_test,Y_pred) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"638a93d1faa0ad7692f552c4aaecc591efce238d"},"cell_type":"code","source":"'''For Both, LabelEncoding and OneHotEncoding -> The accuracy is 25 %'''\nresult_acc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a37a6e3046d207e7da68e4f2539028c950820742"},"cell_type":"markdown","source":"###### Simple Logistic Regression Model\n\n- No of Classes : 2\n- 1 - Rings > 10\n- 0 - Rings <= 10 "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5da276d5cc24c7140d8a07a25df5ab80ec72cfa4"},"cell_type":"code","source":"'''Creating New Target Variable '''\ndf['newRings'] = np.where(df['Rings'] > 10,1,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"18ceecbcce0eb66eaceafedabb8ad78c460c492b"},"cell_type":"code","source":"'''Learning Features and Predicting Features'''\nXtrain = df.drop(['newRings','Rings','Age','Sex'], axis = 1)\nYtrain = df['newRings']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7133b59d1bab3baa0b627082cc8b9e3159ce6818"},"cell_type":"code","source":"'''Train Test Split , 70:30 Ratio'''\nX_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"789c6294b22fab680e12ec90ef3ea36551ec4e53"},"cell_type":"code","source":"'''Creating Object of LogisticRegression'''\nlogreg = LogisticRegression()\n'''Learning from Training Set'''\nlogreg.fit(X_train, Y_train)\n'''Predicting for Training Set'''\nY_pred = logreg.predict(X_test)\n'''Accuracy Score'''\nresult_acc = accuracy_score(Y_test,Y_pred) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7ed3d73d9403e71535a784c5b8e7e2cd8d8c193f"},"cell_type":"code","source":"result_acc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3719062364134c099411b13cc3d548611a987353"},"cell_type":"markdown","source":"##### Note : If you have Binary Classification, Logistic Regression is able to Boost to Higher Accuracy\n"},{"metadata":{"_uuid":"f550e0312d5a619739f0bb9edfe24a16d990d3cf"},"cell_type":"markdown","source":"##### So to Handle Multi-Class Classification, We can Try SVM Model, as it works well for multi-class and multi-label Classification"},{"metadata":{"_uuid":"49bbd8dd4bc0d5f537f7a72b29035c913f0b6c03"},"cell_type":"markdown","source":"###### Multi-Class Classification : When you have one target Column with 3 or more discreet values to predict, you state the problem as multi-class classification."},{"metadata":{"_uuid":"bd33b1557332bf63cbd2d69037893d139a02f12c"},"cell_type":"markdown","source":"###### We WIll first try with all the 28 classes in the target column, using linear kernel , Regularization parameter value as 1, and gamma 1"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"935c4baa6a30337eeda1fefa9281c602be176d74"},"cell_type":"code","source":"'''Importing SVM from SK-Learn'''\nfrom sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5e2cd0cfc0997c39d1741e850170100589cacf9e"},"cell_type":"code","source":"'''Learning Features and Predicting Features'''\nXtrain = df.drop(['Rings','Age','Sex'], axis = 1)\nYtrain = df['Rings']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2661028f39b392a1a294196cedcd09f1ae2d79fe"},"cell_type":"code","source":"'''Train Test Split , 70:30 Ratio'''\nX_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7f0faa35d0806f1f6eba2ec24c8e49cac17394ba"},"cell_type":"code","source":"'''Creating Object of SVM'''\nsvmModel = svm.SVC(kernel='linear', C=1, gamma=1) \n'''Learning from Training Set'''\nsvmModel.fit(X_train, Y_train)\n'''Predicting for Training Set'''\nY_pred = svmModel.predict(X_test)\n'''Accuracy Score'''\nresult_acc = accuracy_score(Y_test,Y_pred) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"90fcf5af7292663a1a52dce1c4c4002946f7576a"},"cell_type":"code","source":"result_acc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17c6a3ad3a5b863f103d20b6eb571ce1d8267e84"},"cell_type":"markdown","source":"###### We can see, the Model Accuracy has increased with SVM, it is now 37 percent.\n- Lets Try to tweak the model Learning Process and see if the accuracy is increases or not."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a4a2390c8d54c932c9c0fc36c757fecba5576ace"},"cell_type":"code","source":"'''Creating Object of SVM'''\nsvmModel = svm.SVC(kernel='rbf', C=1, gamma=100) \n'''Learning from Training Set'''\nsvmModel.fit(X_train, Y_train)\n'''Predicting for Training Set'''\nY_pred = svmModel.predict(X_test)\n'''Accuracy Score'''\nresult_acc = accuracy_score(Y_test,Y_pred) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8cf231aa36854a6e7c236cf84ea989abd33880a7"},"cell_type":"code","source":"result_acc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d720603cade6f913acc9df6d92efca9ff29afbb9"},"cell_type":"markdown","source":"###### We can see, the Model Accuracy has increased with Tweaking SVM parameters, it is now 38 percent.\n- Lets Try to reduce the number of classes and see how the model is performing"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"58e373f5f886ca51d0373ae7f921142ed867f7e5"},"cell_type":"code","source":"'''Making a Copy of the primary DataSet'''\nnew_df = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d38f0e14cdb0400d9f953934d3db3265efd70e56"},"cell_type":"code","source":"'''Feature Engineering , class 1 - 1-8, class 2 - 9-8, class 3 - 11 >'''\nnew_df['newRings_1'] = np.where(df['Rings'] <= 8,1,0)\nnew_df['newRings_2'] = np.where(((df['Rings'] > 8) & (df['Rings'] <= 10)), 2,0)\nnew_df['newRings_3'] = np.where(df['Rings'] > 10,3,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3efc08fea01a1eeda4938e5b0e7a191a7a61537e"},"cell_type":"code","source":"new_df['newRings'] = new_df['newRings_1'] + new_df['newRings_2'] + new_df['newRings_3']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cbf56af5ac577bf0bf5951e5d114f2cedbdc1c5d"},"cell_type":"code","source":"'''Learning Features and Predicting Features'''\nXtrain = new_df.drop(['Rings','Age','Sex','newRings_1','newRings_2','newRings_3'], axis = 1)\nYtrain = new_df['newRings']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"99c115a47d1d1476bd2df83067853a86cf31798d"},"cell_type":"code","source":"'''Train Test Split , 70:30 Ratio'''\nX_train, X_test, Y_train, Y_test = train_test_split(Xtrain, Ytrain, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7511bb7c81ede0cb52003f72074d121200e104e1"},"cell_type":"code","source":"'''Creating Object of SVM'''\nsvmModel = svm.SVC(kernel='rbf', C=1, gamma=100) \n'''Learning from Training Set'''\nsvmModel.fit(X_train, Y_train)\n'''Predicting for Training Set'''\nY_pred = svmModel.predict(X_test)\n'''Accuracy Score'''\nresult_acc = accuracy_score(Y_test,Y_pred) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"545a8413c46846dc46f18e53a9b153f8f4ec467b"},"cell_type":"code","source":"result_acc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5637004368c481ac01431dcba89cbd09f27591fd"},"cell_type":"markdown","source":"###### Final Conclusion : we have not removed Outliers ( as we ad to capture all the type of different shapes and weights of abalone ), But with Less number of classes, SVM is giving an accuracy of 98% ( not Fully Tested ). "},{"metadata":{"_uuid":"a4d883e7c15733709b9d26bd2e7c41f1b8f1e985"},"cell_type":"markdown","source":"###### References Used:\n\n- SVM -> https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n- Abalone DataSet -> https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/\n- Logistic Regression -> Andrews Ng\n\n###### Things To DO: \n- Outlier Data Handling ( to be kept or Removed)\n- More Visulization for Outlier Data\n- Model Output VIsualization\n- More Tweaks on C and Gamma Parameter"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}